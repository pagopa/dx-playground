name: "GitHub Models AI Runner"
description: "Execute prompts using GitHub Models API and return the results"
author: "PagoPA DX Team"

inputs:
  prompt:
    description: "The prompt to send to GitHub Models"
    required: true
  model:
    description: "The model to use (e.g., gpt-4o, gpt-4o-mini, o1-preview, o1-mini)"
    required: false
    default: "gpt-4o-mini"
  github-token:
    description: "GitHub token for authentication. Defaults to GITHUB_TOKEN"
    required: false
    default: ${{ github.token }}
  max-tokens:
    description: "Maximum tokens in the response"
    required: false
    default: "4096"
  temperature:
    description: "Temperature for response generation (0.0-2.0)"
    required: false
    default: "1.0"

outputs:
  result:
    description: "The response from GitHub Models"
    value: ${{ steps.run-model.outputs.result }}
  success:
    description: "Whether the command executed successfully (true/false)"
    value: ${{ steps.run-model.outputs.success }}

runs:
  using: "composite"
  steps:
    - name: Call GitHub Models API
      id: run-model
      uses: actions/github-script@v7
      env:
        PROMPT: ${{ inputs.prompt }}
        MODEL: ${{ inputs.model }}
        MAX_TOKENS: ${{ inputs.max-tokens }}
        TEMPERATURE: ${{ inputs.temperature }}
      with:
        github-token: ${{ inputs.github-token }}
        script: |
          const prompt = process.env.PROMPT;
          const model = process.env.MODEL;
          const maxTokens = parseInt(process.env.MAX_TOKENS);
          const temperature = parseFloat(process.env.TEMPERATURE);

          try {
            core.info(`Calling GitHub Models API with model: ${model}`);

            const response = await fetch('https://models.inference.ai.azure.com/chat/completions', {
              method: 'POST',
              headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${process.env.GITHUB_TOKEN}`
              },
              body: JSON.stringify({
                messages: [
                  {
                    role: 'user',
                    content: prompt
                  }
                ],
                model: model,
                temperature: temperature,
                max_tokens: maxTokens
              })
            });

            if (!response.ok) {
              const errorText = await response.text();
              throw new Error(`GitHub Models API error (${response.status}): ${errorText}`);
            }

            const data = await response.json();

            if (!data.choices || data.choices.length === 0) {
              throw new Error('No response from GitHub Models API');
            }

            const result = data.choices[0].message.content;

            core.setOutput('success', 'true');
            core.setOutput('result', result);
            core.info('GitHub Models API call successful');

          } catch (error) {
            core.error(`GitHub Models API failed: ${error.message}`);
            core.setOutput('success', 'false');
            core.setOutput('result', error.message);
          }

branding:
  icon: "cpu"
  color: "purple"
